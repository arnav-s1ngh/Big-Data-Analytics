{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4d1295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9703149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: neo4j.url\n",
      "Warning: Ignoring non-Spark config property: neo4j.database\n",
      "Warning: Ignoring non-Spark config property: neo4j.authentication.basic.password\n",
      "Warning: Ignoring non-Spark config property: neo4j.authentication.basic.username\n",
      "24/11/15 02:25:02 WARN Utils: Your hostname, Alice-In-Chains.local resolves to a loopback address: 127.0.0.1; using 192.168.48.70 instead (on interface en0)\n",
      "24/11/15 02:25:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/arnavsingh/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/arnavsingh/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/arnavsingh/.ivy2/jars\n",
      "org.neo4j#neo4j-connector-apache-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f7f789fe-1a11-454c-87f7-42634cb5b0ee;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.12;5.3.2_for_spark_3 in central\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.12_common;5.3.2_for_spark_3 in central\n",
      "\tfound org.neo4j#neo4j-cypher-dsl;2022.11.0 in central\n",
      "\tfound org.apiguardian#apiguardian-api;1.1.2 in local-m2-cache\n",
      "\tfound org.neo4j.driver#neo4j-java-driver;4.4.18 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      ":: resolution report :: resolve 181ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\torg.apiguardian#apiguardian-api;1.1.2 from local-m2-cache in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.12;5.3.2_for_spark_3 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.12_common;5.3.2_for_spark_3 from central in [default]\n",
      "\torg.neo4j#neo4j-cypher-dsl;2022.11.0 from central in [default]\n",
      "\torg.neo4j.driver#neo4j-java-driver;4.4.18 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f7f789fe-1a11-454c-87f7-42634cb5b0ee\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/4ms)\n",
      "24/11/15 02:25:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=(\n",
    "    SparkSession.builder.config(\"neo4j.url\", \"neo4j://localhost:7687\")\n",
    "    .appName(\"BDA Assignment 3\")\n",
    "    .config(\"spark.driver.memory\", \"15g\")\n",
    "    .config(\"spark.jars.packages\",\"org.neo4j:neo4j-connector-apache-spark_2.12:5.3.2_for_spark_3\")\n",
    "    .config(\"neo4j.authentication.basic.username\", \"neo4j\")\n",
    "    .config(\"neo4j.authentication.basic.password\", \"12345678\")\n",
    "    .config(\"neo4j.database\", \"assignmentbda3\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "node_df=(\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"labels\",\"Paper\")\n",
    "    .load()\n",
    ")\n",
    "query = \"\"\"\n",
    "MATCH (p1:Paper)-[:cite]->(p2:Paper)\n",
    "RETURN p1.id AS citing_paper, p2.id AS cited_paper\n",
    "\"\"\"\n",
    "\n",
    "# Run the query on Neo4j and load the result into a DataFrame\n",
    "edge_df = (\n",
    "    spark.read.format(\"org.neo4j.spark.DataSource\")\n",
    "    .option(\"query\", query)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72120a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "edges = edge_df.rdd.map(lambda row: (row['citing_paper'], row['cited_paper'])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfeb227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60de695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b32380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_nodes = [\"2982615777\", \"1556418098\"]\n",
    "c_values = [0.7, 0.8, 0.9]\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aac2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with c = 0.7\n",
      "Processing node 2982615777\n",
      "Processing node 1556418098\n",
      "\n",
      "Processing with c = 0.8\n",
      "Processing node 2982615777\n",
      "Processing node 1556418098\n",
      "\n",
      "Processing with c = 0.9\n",
      "Processing node 2982615777\n",
      "Processing node 1556418098\n",
      "+----------+------------+-----------+-------+\n",
      "|query_node|similar_node|similarity |c_value|\n",
      "+----------+------------+-----------+-------+\n",
      "|1556418098|1606626619  |0.2738696  |0.7    |\n",
      "|1556418098|2150185515  |0.24748737 |0.7    |\n",
      "|1556418098|1979504468  |0.24748737 |0.7    |\n",
      "|1556418098|2051613311  |0.24748737 |0.7    |\n",
      "|1556418098|2145343602  |0.24748737 |0.7    |\n",
      "|1556418098|1606626619  |0.31299385 |0.8    |\n",
      "|1556418098|1979504468  |0.28284273 |0.8    |\n",
      "|1556418098|2150185515  |0.28284273 |0.8    |\n",
      "|1556418098|2051613311  |0.28284273 |0.8    |\n",
      "|1556418098|2145343602  |0.28284273 |0.8    |\n",
      "|1556418098|1606626619  |0.35211807 |0.9    |\n",
      "|1556418098|2145343602  |0.31819806 |0.9    |\n",
      "|1556418098|1979504468  |0.31819806 |0.9    |\n",
      "|1556418098|2051613311  |0.31819806 |0.9    |\n",
      "|1556418098|2150185515  |0.31819806 |0.9    |\n",
      "|2982615777|2410108711  |0.11514155 |0.7    |\n",
      "|2982615777|2982519456  |0.110021554|0.7    |\n",
      "|2982615777|1493057577  |0.10552897 |0.7    |\n",
      "|2982615777|2922574803  |0.10552897 |0.7    |\n",
      "|2982615777|1483259594  |0.10552897 |0.7    |\n",
      "+----------+------------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, Row\n",
    "import gc\n",
    "\n",
    "def get_local_neighborhood(G, node, max_depth=10):\n",
    "    \"\"\"Get local neighborhood of a node up to max_depth\"\"\"\n",
    "    neighborhood = {node}\n",
    "    current_level = {node}\n",
    "    \n",
    "    for _ in range(max_depth):\n",
    "        next_level = set()\n",
    "        for n in current_level:\n",
    "            # Get both predecessors and successors\n",
    "            neighbors = set(G.predecessors(n)) | set(G.successors(n))\n",
    "            next_level.update(neighbors)\n",
    "        current_level = next_level - neighborhood\n",
    "        neighborhood.update(next_level)\n",
    "        if not current_level:\n",
    "            break\n",
    "    \n",
    "    return neighborhood\n",
    "\n",
    "def local_simrank_score(G, node1, node2, c=0.8, max_depth=10):\n",
    "    \"\"\"Compute SimRank score based only on local structure\"\"\"\n",
    "    if node1 == node2:\n",
    "        return 1.0\n",
    "        \n",
    "    # Get immediate predecessors\n",
    "    pred1 = set(G.predecessors(node1))\n",
    "    pred2 = set(G.predecessors(node2))\n",
    "    \n",
    "    # Get immediate successors\n",
    "    succ1 = set(G.successors(node1))\n",
    "    succ2 = set(G.successors(node2))\n",
    "    \n",
    "    # Calculate similarity based on common neighbors\n",
    "    pred_similarity = len(pred1 & pred2) / (len(pred1) * len(pred2))**0.5 if pred1 and pred2 else 0\n",
    "    succ_similarity = len(succ1 & succ2) / (len(succ1) * len(succ2))**0.5 if succ1 and succ2 else 0\n",
    "    \n",
    "    return c * (pred_similarity + succ_similarity) / 2\n",
    "\n",
    "def find_similar_nodes_local(G, query_node, c=0.8, top_k=5, max_depth=10):\n",
    "    \"\"\"Find similar nodes using only local neighborhood analysis\"\"\"\n",
    "    try:\n",
    "        # Get local neighborhood\n",
    "        local_nodes = get_local_neighborhood(G, query_node, max_depth)\n",
    "        \n",
    "        # Remove the query node itself\n",
    "        local_nodes.remove(query_node)\n",
    "        \n",
    "        # Calculate similarities only for local neighborhood\n",
    "        similarities = []\n",
    "        for other_node in local_nodes:\n",
    "            sim = local_simrank_score(G, query_node, other_node, c, max_depth)\n",
    "            if sim > 0:  # Only keep non-zero similarities\n",
    "                similarities.append((other_node, sim))\n",
    "        \n",
    "        # Get top-k results\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing node {query_node}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def process_nodes_in_chunks(G, query_nodes, c_values, top_k=5, chunk_size=10):\n",
    "    \"\"\"Process nodes in small chunks to manage memory\"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"query_node\", StringType(), False),\n",
    "        StructField(\"similar_node\", StringType(), False),\n",
    "        StructField(\"similarity\", FloatType(), False),\n",
    "        StructField(\"c_value\", FloatType(), False)\n",
    "    ])\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    try:\n",
    "        for c in c_values:\n",
    "            print(f\"\\nProcessing with c = {c}\")\n",
    "            \n",
    "            for i in range(0, len(query_nodes), chunk_size):\n",
    "                chunk = query_nodes[i:i + chunk_size]\n",
    "                chunk_results = []\n",
    "                \n",
    "                for node in chunk:\n",
    "                    print(f\"Processing node {node}\")\n",
    "                    similar_nodes = find_similar_nodes_local(G, node, c, top_k)\n",
    "                    \n",
    "                    for similar_node, similarity in similar_nodes:\n",
    "                        chunk_results.append(Row(\n",
    "                            query_node=node,\n",
    "                            similar_node=similar_node,\n",
    "                            similarity=float(similarity),\n",
    "                            c_value=c\n",
    "                        ))\n",
    "                \n",
    "                # Convert chunk results to DataFrame\n",
    "                if chunk_results:\n",
    "                    chunk_df = spark.createDataFrame(chunk_results, schema)\n",
    "                    if not all_results:\n",
    "                        all_results = chunk_df\n",
    "                    else:\n",
    "                        all_results = all_results.unionAll(chunk_df)\n",
    "                \n",
    "                # Force computation and cache\n",
    "                if all_results:\n",
    "                    all_results = all_results.cache()\n",
    "                    all_results.count()\n",
    "                \n",
    "                # Clear memory\n",
    "                gc.collect()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in chunk processing: {str(e)}\")\n",
    "        if all_results:\n",
    "            return all_results\n",
    "        return spark.createDataFrame([], schema)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Usage\n",
    "try:\n",
    "    # Process with very conservative parameters\n",
    "    final_df = process_nodes_in_chunks(\n",
    "        G=G,\n",
    "        query_nodes=query_nodes,\n",
    "        c_values=c_values,\n",
    "        top_k=5,\n",
    "        chunk_size=1  # Process one node at a time\n",
    "    )\n",
    "    \n",
    "    # Show results sorted by similarity\n",
    "    final_df = final_df.orderBy(\n",
    "        [\"query_node\", \"c_value\", \"similarity\"], \n",
    "        ascending=[True, True, False]\n",
    "    )\n",
    "    final_df.show(truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Global error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa786bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
